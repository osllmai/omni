{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT explainer for income prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath('E:/Codes/OmniXAI/')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# For Jupyter notebooks or interactive environments where __file__ is not defined\n",
    "try:\n",
    "    # Try to use __file__ if available\n",
    "    directory = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # If __file__ is not defined (e.g., in Jupyter), use the current working directory\n",
    "    directory = os.path.abspath('')\n",
    "    \n",
    "sys.path.append(os.path.dirname(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from omnixai.data.tabular import Tabular\n",
    "from omnixai.preprocessing.tabular import TabularTransform\n",
    "from omnixai.explainers.tabular import LLMExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diabetes_data(file_path):\n",
    "    \"\"\"Load and preprocess the diabetes dataset\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get feature names (all columns except 'class')\n",
    "    feature_names = [col for col in df.columns if col != 'class']\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df[feature_names].copy()\n",
    "    y = df['class'].copy()\n",
    "    \n",
    "    # Encode categorical features\n",
    "    label_encoders = {}\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or col == 'Gender':\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Encode target variable (Positive=1, Negative=0)\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y)\n",
    "    \n",
    "    # Split into train/test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X.values, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "def train_tf_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"Train a TensorFlow model for diabetes prediction\"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import layers, models\n",
    "        from tensorflow.keras.utils import to_categorical\n",
    "        \n",
    "        # Convert to categorical if needed\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        if num_classes == 2:\n",
    "            # Binary classification\n",
    "            y_train_cat = y_train.reshape(-1, 1)\n",
    "            y_test_cat = y_test.reshape(-1, 1)\n",
    "        else:\n",
    "            # Multi-class classification\n",
    "            y_train_cat = to_categorical(y_train, num_classes)\n",
    "            y_test_cat = to_categorical(y_test, num_classes)\n",
    "        \n",
    "        # Create the model\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(1 if num_classes == 2 else num_classes, \n",
    "                        activation='sigmoid' if num_classes == 2 else 'softmax')\n",
    "        ])\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy' if num_classes == 2 else 'categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            x_train, y_train_cat,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_data=(x_test, y_test_cat),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model\n",
    "        train_loss, train_acc = model.evaluate(x_train, y_train_cat, verbose=0)\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "        \n",
    "        print(f\"Train loss: {train_loss:.4f}, train accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Test loss:  {test_loss:.4f}, test accuracy:  {test_acc:.4f}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"TensorFlow not available, using XGBoost instead\")\n",
    "        import xgboost as xgb\n",
    "        \n",
    "        # Use XGBoost as fallback\n",
    "        model = xgb.XGBClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        train_acc = model.score(x_train, y_train)\n",
    "        test_acc = model.score(x_test, y_test)\n",
    "        \n",
    "        print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Test accuracy:  {test_acc:.4f}\")\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (416, 16)\n",
      "x_test shape:  (104, 16)\n",
      "TensorFlow not available, using XGBoost instead\n",
      "Train accuracy: 1.0000\n",
      "Test accuracy:  0.9808\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/diabetes.csv'\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = diabetes_data(file_path)\n",
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('x_test shape:  {}'.format(x_test.shape))\n",
    "\n",
    "model = train_tf_model(x_train, y_train, x_test, y_test)\n",
    "# Used for initializing the explainer\n",
    "tabular_data = Tabular(\n",
    "    x_train,\n",
    "    feature_columns=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this example is for income prediction (https://archive.ics.uci.edu/ml/datasets/adult). We recommend using `Tabular` to represent a tabular dataset, which can be constructed from a pandas dataframe or a numpy array. To create a `Tabular` instance given a numpy array, one needs to specify the data, the feature names, the categorical feature names (if exists) and the target/label column name (if exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age         Workclass  fnlwgt   Education Education-Num  \\\n",
      "0      39         State-gov   77516   Bachelors            13   \n",
      "1      50  Self-emp-not-inc   83311   Bachelors            13   \n",
      "2      38           Private  215646     HS-grad             9   \n",
      "3      53           Private  234721        11th             7   \n",
      "4      28           Private  338409   Bachelors            13   \n",
      "...    ..               ...     ...         ...           ...   \n",
      "32556  27           Private  257302  Assoc-acdm            12   \n",
      "32557  40           Private  154374     HS-grad             9   \n",
      "32558  58           Private  151910     HS-grad             9   \n",
      "32559  22           Private  201490     HS-grad             9   \n",
      "32560  52      Self-emp-inc  287927     HS-grad             9   \n",
      "\n",
      "           Marital Status         Occupation   Relationship   Race     Sex  \\\n",
      "0           Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1      Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2                Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3      Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4      Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "...                   ...                ...            ...    ...     ...   \n",
      "32556  Married-civ-spouse       Tech-support           Wife  White  Female   \n",
      "32557  Married-civ-spouse  Machine-op-inspct        Husband  White    Male   \n",
      "32558             Widowed       Adm-clerical      Unmarried  White  Female   \n",
      "32559       Never-married       Adm-clerical      Own-child  White    Male   \n",
      "32560  Married-civ-spouse    Exec-managerial           Wife  White  Female   \n",
      "\n",
      "      Capital Gain Capital Loss Hours per week        Country  label  \n",
      "0             2174            0             40  United-States  <=50K  \n",
      "1                0            0             13  United-States  <=50K  \n",
      "2                0            0             40  United-States  <=50K  \n",
      "3                0            0             40  United-States  <=50K  \n",
      "4                0            0             40           Cuba  <=50K  \n",
      "...            ...          ...            ...            ...    ...  \n",
      "32556            0            0             38  United-States  <=50K  \n",
      "32557            0            0             40  United-States   >50K  \n",
      "32558            0            0             40  United-States  <=50K  \n",
      "32559            0            0             20  United-States  <=50K  \n",
      "32560        15024            0             40  United-States   >50K  \n",
      "\n",
      "[32561 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_names = [\n",
    "    \"Age\", \"Workclass\", \"fnlwgt\", \"Education\",\n",
    "    \"Education-Num\", \"Marital Status\", \"Occupation\",\n",
    "    \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\",\n",
    "    \"Capital Loss\", \"Hours per week\", \"Country\", \"label\"\n",
    "]\n",
    "data = np.genfromtxt(os.path.join('../data', 'adult.data'), delimiter=', ', dtype=str)\n",
    "tabular_data = Tabular(\n",
    "    data,\n",
    "    feature_columns=feature_names,\n",
    "    categorical_columns=[feature_names[i] for i in [1, 3, 5, 6, 7, 8, 9, 13]],\n",
    "    target_column='label'\n",
    ")\n",
    "print(tabular_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabularTransform` is a special transform designed for tabular data. By default, it converts categorical features into one-hot encoding, and keeps continuous-valued features (if one wants to normalize continuous-valued features, set the parameter `cont_transform` in `TabularTransform` to `Standard` or `MinMax`). The `transform` method of `TabularTransform` will transform a `Tabular` instance into a numpy array. If the `Tabular` instance has a target/label column, the last column of the transformed numpy array will be the target/label. \n",
    "\n",
    "If one wants some other transformations that are not supported in the library, one can simply convert the `Tabular` instance into a pandas dataframe by calling `Tabular.to_pd()` and try different transformations with it.\n",
    "\n",
    "After data preprocessing, we can train a XGBoost classifier for this task (one may try other classifiers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (26048, 108)\n",
      "Test data shape:     (6513, 108)\n",
      "Test accuracy: 0.865806847842776\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "transformer = TabularTransform().fit(tabular_data)\n",
    "class_names = transformer.class_names\n",
    "x = transformer.transform(tabular_data)\n",
    "train, test, labels_train, labels_test = \\\n",
    "    sklearn.model_selection.train_test_split(x[:, :-1], x[:, -1], train_size=0.80)\n",
    "print('Training data shape: {}'.format(train.shape))\n",
    "print('Test data shape:     {}'.format(test.shape))\n",
    "\n",
    "gbtree = xgboost.XGBClassifier(n_estimators=300, max_depth=5)\n",
    "gbtree.fit(train, labels_train)\n",
    "print('Test accuracy: {}'.format(\n",
    "    sklearn.metrics.accuracy_score(labels_test, gbtree.predict(test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction function takes a `Tabular` instance as its inputs, and outputs the class probabilities for classification tasks or the estimated values for regression tasks. In this example, we simply call `transformer.transform` to do data preprocessing followed by the prediction function of `gbtree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_function=lambda z: gbtree.predict_proba(transformer.transform(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize a GPT explainer, we need to set:\n",
    "  \n",
    "  - `training_data`: The data used to initialize a SHAP explainer. ``training_data`` can be the training dataset for training the machine learning model. If the training dataset is too large, ``training_data`` can be a subset of it by applying `omnixai.sampler.tabular.Sampler.subsample`.\n",
    "  - `predict_function`: The prediction function corresponding to the model.\n",
    "  - `mode`: The task type, e.g., \"classification\" or \"regression\".\n",
    "  - `apikey`: The OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 150 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    }
   ],
   "source": [
    "explainer = LLMExplainer(\n",
    "    training_data=tabular_data,\n",
    "    predict_function=predict_function,\n",
    "    indoxrouter_apikey=\"indox-e3tqyXlz0lctgM6tZdjzcKQopuc4QoLF\",\n",
    "    indoxrouter_model=\"openai/gpt-4o-mini\"\n",
    ")\n",
    "# Apply an inverse transform, i.e., converting the numpy array back to `Tabular`\n",
    "test_instances = transformer.invert(test)\n",
    "test_x = test_instances[1653]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to generate explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4693bc3b5e834b4cb7683dabda32bfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This example is classified as label_1 primarily due to the high feature importance of \"Capital Gain = 15024.0\", which has the highest score of 7.0389. This indicates that capital gain is a strong predictor for the positive label. Other contributing factors include age, education level, work class, and marital status, all of which have positive feature importance scores, suggesting they also contribute to the prediction of label_1.\n",
      "\n",
      "To change the predicted label from label_1 to label_0, you can adjust the feature values as follows: set \"Capital Gain\" to \"3756.0\" and \"Capital Loss\" to \"125.125\". This modification alters the input in a way that the model predicts label_0 instead of label_1.\n"
     ]
    }
   ],
   "source": [
    "explanations = explainer.explain(test_x)\n",
    "print(explanations.get_explanations(index=0)[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
